import cv2
import mediapipe as mp
import time

from enum import Enum

class FaceRegion(Enum):
    LEFT_EYE = 1
    RIGHT_EYE = 2
    LEFT_EYEBROW = 3
    RIGHT_EYEBROW = 4
    NOSE = 5
    UPPER_LIP = 6
    LOWER_LIP = 7
    INSIDE_LIP = 8

class MediapipeKPDetector():

    def __init__(self, staticMode=False, maxFaces=1, refine_landmarks=True, minDetectionCon=0.4):

        self.staticMode = staticMode
        self.maxFaces = maxFaces
        self.refine_landmarks = refine_landmarks
        self.minDetectionCon = minDetectionCon

        self.mpDraw = mp.solutions.drawing_utils
        self.mpFaceMesh = mp.solutions.face_mesh

        self.faceMesh = self.mpFaceMesh.FaceMesh(self.staticMode, self.maxFaces, self.refine_landmarks, self.minDetectionCon)
        self.drawSpec = self.mpDraw.DrawingSpec(thickness=1, circle_radius=2)

        # Load filtered indices
        self.filtered_ids = self.get_68KP_indices()
    
    @staticmethod
    def get_68KP_indices(as_dict=False):
        """
        This function return a list with Keypoints 17-67 of the 68 keypoint model (generated by mediapipe)
        Filters the 468 points from mediapipe FaceMesh

        Keypoints 0-16 are the face contour and are not relevant

        View mesh annotation at:
        https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/src/mediapipe-facemesh/keypoints.ts#L49
        """

        # Mapped to [36, 37, 38, 39, 40, 41]
        left_eye_ids = [33, 160, 158, 133, 153, 144]

        # Mapped to [42, 43, 44, 45, 46, 47]
        right_eye_ids = [362, 385, 387, 263, 373, 380]

        # Mapped to [27, 28, 29, 30, 31, 32, 33, 34, 35]
        nose_ids = [6, 195, 5, 4, 240, 97, 2, 326, 460]

        # Removed 55, 193, 156
        # Mapped to [17, 18, 19, 20, 21]
        left_eyebrow_ids = [70, 63, 105, 66, 107]

        # Removed 383, 285, 417
        # Mapped to [22, 23, 24, 25, 26]
        right_eyebrow_ids = [336, 296, 334, 293, 300] # still have to remove 1

        # Mapped to [48, 49, 50, 51, 52, 53, 54]
        lip_upper_ids = [61, 40, 37, 0, 267, 270, 291]

        # Mapped to [60, 61, 62, 63, 64, 65, 66, 67]
        lip_inside_ids = [96, 82, 13, 312, 325, 317, 14, 87]

        # Mapped to [55, 56, 57, 58, 59]
        lip_lower_ids = [91, 84, 17, 314, 321]

        if not as_dict:
            master_ids = sum([
                left_eyebrow_ids,
                right_eyebrow_ids,
                nose_ids,
                left_eye_ids,
                right_eye_ids,
                lip_upper_ids, 
                lip_lower_ids, 
                lip_inside_ids
            ], [])

            return master_ids
        
        else:
            master_ids = dict()

            master_ids[FaceRegion.LEFT_EYE] = left_eye_ids
            master_ids[FaceRegion.RIGHT_EYE] = right_eye_ids

            master_ids[FaceRegion.LEFT_EYEBROW] = left_eyebrow_ids
            master_ids[FaceRegion.RIGHT_EYEBROW] = right_eyebrow_ids

            master_ids[FaceRegion.UPPER_LIP] = lip_upper_ids
            master_ids[FaceRegion.LOWER_LIP] = lip_lower_ids
            master_ids[FaceRegion.INSIDE_LIP] = lip_inside_ids

            master_ids[FaceRegion.NOSE] = nose_ids

            return master_ids
    
    @staticmethod
    def mediapipe_annotations():
        # Source: https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/src/mediapipe-facemesh/keypoints.ts#L49

        return {
            'silhouette': [
                10,  338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,\
                397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,\
                172, 58,  132, 93,  234, 127, 162, 21,  54,  103, 67,  109\
            ],

            'lipsUpperOuter': [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291],
            'lipsLowerOuter': [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291],
            'lipsUpperInner': [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308],
            'lipsLowerInner': [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308],

            'rightEyeUpper0': [246, 161, 160, 159, 158, 157, 173],
            'rightEyeLower0': [33, 7, 163, 144, 145, 153, 154, 155, 133],
            'rightEyeUpper1': [247, 30, 29, 27, 28, 56, 190],
            'rightEyeLower1': [130, 25, 110, 24, 23, 22, 26, 112, 243],
            'rightEyeUpper2': [113, 225, 224, 223, 222, 221, 189],
            'rightEyeLower2': [226, 31, 228, 229, 230, 231, 232, 233, 244],
            'rightEyeLower3': [143, 111, 117, 118, 119, 120, 121, 128, 245],

            'rightEyebrowUpper': [156, 70, 63, 105, 66, 107, 55, 193],
            'rightEyebrowLower': [35, 124, 46, 53, 52, 65],

            'rightEyeIris': [473, 474, 475, 476, 477],

            'leftEyeUpper0': [466, 388, 387, 386, 385, 384, 398],
            'leftEyeLower0': [263, 249, 390, 373, 374, 380, 381, 382, 362],
            'leftEyeUpper1': [467, 260, 259, 257, 258, 286, 414],
            'leftEyeLower1': [359, 255, 339, 254, 253, 252, 256, 341, 463],
            'leftEyeUpper2': [342, 445, 444, 443, 442, 441, 413],
            'leftEyeLower2': [446, 261, 448, 449, 450, 451, 452, 453, 464],
            'leftEyeLower3': [372, 340, 346, 347, 348, 349, 350, 357, 465],

            'leftEyebrowUpper': [383, 300, 293, 334, 296, 336, 285, 417],
            'leftEyebrowLower': [265, 353, 276, 283, 282, 295],

            'leftEyeIris': [468, 469, 470, 471, 472],

            'midwayBetweenEyes': [168],

            'noseTip': [1],
            'noseBottom': [2],
            'noseRightCorner': [98],
            'noseLeftCorner': [327],

            'rightCheek': [205],
            'leftCheek': [425]
        }

    def findFaceMesh(self, img, draw_points=True, draw_indices=False, filtered=False):
        self.imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        self.results = self.faceMesh.process(self.imgRGB)

        faces = []

        if self.results.multi_face_landmarks:

            # Every faceLms corresponds to the landmarks of one face
            for faceLms in self.results.multi_face_landmarks:
                face = []

                for id,lm in enumerate(faceLms.landmark):

                    # Landmark x,y,z is normalized
                    # Convert them back by remultiplying width/height
                    ih, iw, ic = img.shape
                    x,y = int(lm.x*iw), int(lm.y*ih)

                    circle_radius = 2 if ih < 1000 else 6

                    if draw_points:
                        if filtered:
                            if id in self.filtered_ids:
                                if draw_indices:
                                    cv2.putText(img, str(id), (x, y), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 0), 1)
                                else:
                                    cv2.circle(img, (x,y), circle_radius, (255,0,0), cv2.FILLED)
                                face.append([x,y])
                        else:
                            if draw_indices:
                                cv2.putText(img, str(id), (x, y), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 0), 1)
                            else:
                                cv2.circle(img, (x,y), circle_radius, (255,0,0), cv2.FILLED)

                    if filtered and id in self.filtered_ids or not filtered:
                        face.append([x,y])


                faces.append(face)

                # Draw (filtered) landmarks
                if not draw_points:
                    self.mpDraw.draw_landmarks(img, faceLms, self.mpFaceMesh.FACEMESH_CONTOURS, self.drawSpec, self.drawSpec)

        return img, faces

# Sample usage
def main():
    use_video = False

    # Load the resource
    cap = cv2.VideoCapture(0) if use_video else cv2.imread('../../images/paralysis_test.jpg') 
    cap = cv2.imread('/home/robbedec/repos/ugent/thesis-inwe/data/MEEI_Standard_Set/Flaccid/MildFlaccid/MildFlaccid1/MildFlaccid1_1.jpg')
    pTime = 0
    detector = MediapipeKPDetector(maxFaces=1)

    while True:
        if use_video:
            success, img = cap.read()
        else:
            img = cap

        # Call landmark generator
        img, faces = detector.findFaceMesh(img, draw_points=True, draw_indices=True, filtered=False)

        if len(faces)!= 0:
            print(faces[0])

        # Calcultate FPS
        if use_video:
            cTime = time.time()
            fps = 1 / (cTime - pTime)
            pTime = cTime
            cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_PLAIN,
                        3, (0, 255, 0), 3)

        cv2.imshow("Image", img)

        if use_video:
            cv2.waitKey(1)
        else:
            cv2.waitKey(0)
            break

if __name__ == "__main__":
    main()