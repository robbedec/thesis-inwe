import cv2
import mediapipe as mp
import time
import numpy as np
import math

from enum import Enum

class FaceRegion(Enum):
    LEFT_EYE = 1
    RIGHT_EYE = 2
    LEFT_EYEBROW = 3
    RIGHT_EYEBROW = 4
    NOSE = 5
    UPPER_LIP = 6
    LOWER_LIP = 7
    INSIDE_LIP = 8

class MediapipeKPDetector():

    def __init__(self, staticMode=False, maxFaces=1, refine_landmarks=True, minDetectionCon=0.4):

        self.staticMode = staticMode
        self.maxFaces = maxFaces
        self.refine_landmarks = refine_landmarks
        self.minDetectionCon = minDetectionCon

        self.mpDraw = mp.solutions.drawing_utils
        self.mpFaceMesh = mp.solutions.face_mesh

        self.faceMesh = self.mpFaceMesh.FaceMesh(self.staticMode, self.maxFaces, self.refine_landmarks, self.minDetectionCon)
        self.drawSpec = self.mpDraw.DrawingSpec(thickness=1, circle_radius=2)

        # Load filtered indices
        self.filtered_ids = self.get_68KP_indices()
    
    @staticmethod
    def get_68KP_indices(as_dict=False):
        """
        This function return a list with Keypoints 17-67 of the 68 keypoint model (generated by mediapipe)
        Filters the 468 points from mediapipe FaceMesh

        Keypoints 0-16 are the face contour and are not relevant

        View mesh annotation at:
        https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/src/mediapipe-facemesh/keypoints.ts#L49
        """

        # Mapped to [36, 37, 38, 39, 40, 41]
        left_eye_ids = [33, 160, 158, 133, 153, 144]

        # Mapped to [42, 43, 44, 45, 46, 47]
        right_eye_ids = [362, 385, 387, 263, 373, 380]

        # Mapped to [27, 28, 29, 30, 31, 32, 33, 34, 35]
        nose_ids = [6, 195, 5, 4, 240, 97, 2, 326, 460]

        # Removed 55, 193, 156
        # Mapped to [17, 18, 19, 20, 21]
        left_eyebrow_ids = [70, 63, 105, 66, 107]

        # Removed 383, 285, 417
        # Mapped to [22, 23, 24, 25, 26]
        right_eyebrow_ids = [336, 296, 334, 293, 300] # still have to remove 1

        # Mapped to [48, 49, 50, 51, 52, 53, 54]
        lip_upper_ids = [61, 40, 37, 0, 267, 270, 291]

        # Mapped to [60, 61, 62, 63, 64, 65, 66, 67]
        lip_inside_ids = [96, 82, 13, 312, 325, 317, 14, 87]

        # Mapped to [55, 56, 57, 58, 59]
        lip_lower_ids = [91, 84, 17, 314, 321]

        if not as_dict:
            master_ids = sum([
                left_eyebrow_ids,
                right_eyebrow_ids,
                nose_ids,
                left_eye_ids,
                right_eye_ids,
                lip_upper_ids, 
                lip_lower_ids, 
                lip_inside_ids
            ], [])

            return master_ids
        
        else:
            master_ids = dict()

            master_ids[FaceRegion.LEFT_EYE] = left_eye_ids
            master_ids[FaceRegion.RIGHT_EYE] = right_eye_ids

            master_ids[FaceRegion.LEFT_EYEBROW] = left_eyebrow_ids
            master_ids[FaceRegion.RIGHT_EYEBROW] = right_eyebrow_ids

            master_ids[FaceRegion.UPPER_LIP] = lip_upper_ids
            master_ids[FaceRegion.LOWER_LIP] = lip_lower_ids
            master_ids[FaceRegion.INSIDE_LIP] = lip_inside_ids

            master_ids[FaceRegion.NOSE] = nose_ids

            return master_ids
    
    @staticmethod
    def mediapipe_annotations():
        # Source: https://github.com/tensorflow/tfjs-models/blob/master/face-landmarks-detection/src/mediapipe-facemesh/keypoints.ts#L49

        return {
            'silhouette': [
                10,  338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,\
                397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,\
                172, 58,  132, 93,  234, 127, 162, 21,  54,  103, 67,  109\
            ],

            'lipsUpperOuter': [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291],
            'lipsLowerOuter': [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291],
            'lipsUpperInner': [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308],
            'lipsLowerInner': [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308],

            'rightEyeUpper0': [246, 161, 160, 159, 158, 157, 173],
            'rightEyeLower0': [33, 7, 163, 144, 145, 153, 154, 155, 133],
            'rightEyeUpper1': [247, 30, 29, 27, 28, 56, 190],
            'rightEyeLower1': [130, 25, 110, 24, 23, 22, 26, 112, 243],
            'rightEyeUpper2': [113, 225, 224, 223, 222, 221, 189],
            'rightEyeLower2': [226, 31, 228, 229, 230, 231, 232, 233, 244],
            'rightEyeLower3': [143, 111, 117, 118, 119, 120, 121, 128, 245],

            'rightEyebrowUpper': [156, 70, 63, 105, 66, 107, 55, 193],
            'rightEyebrowLower': [35, 124, 46, 53, 52, 65],

            'rightEyeIris': [473, 474, 475, 476, 477],

            'leftEyeUpper0': [466, 388, 387, 386, 385, 384, 398],
            'leftEyeLower0': [263, 249, 390, 373, 374, 380, 381, 382, 362],
            'leftEyeUpper1': [467, 260, 259, 257, 258, 286, 414],
            'leftEyeLower1': [359, 255, 339, 254, 253, 252, 256, 341, 463],
            'leftEyeUpper2': [342, 445, 444, 443, 442, 441, 413],
            'leftEyeLower2': [446, 261, 448, 449, 450, 451, 452, 453, 464],
            'leftEyeLower3': [372, 340, 346, 347, 348, 349, 350, 357, 465],

            'leftEyebrowUpper': [383, 300, 293, 334, 296, 336, 285, 417],
            'leftEyebrowLower': [265, 353, 276, 283, 282, 295],

            'leftEyeIris': [468, 469, 470, 471, 472],

            'midwayBetweenEyes': [168],

            'noseTip': [1],
            'noseBottom': [2],
            'noseRightCorner': [98],
            'noseLeftCorner': [327],

            'rightCheek': [205],
            'leftCheek': [425]
        }

    def findFaceMesh(self, img, draw_points=True, draw_indices=False, filtered=False, pose_estimation=False):
        """
        - draw_points: Draw circles on the keypoints instead of drawing the complete mesh.

        - draw_indices: Draw the index of the keypoint instead of a circle.

        - filtered: Only extract the 51 relevant keypoints from the 68KP model
                    (face contour excluded, see self.get_68KP_indices). 
        
        - pose_estimation: Instructs the algorithm to also calculate the orientation of the 
                           face. The result for eacht detected face in the image is stored in
                           the _orientation_angles property. Each entry contains three euler
                           angles (roll, pitch, yaw).
        """

        self.imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        self.results = self.faceMesh.process(self.imgRGB)
        self._img = img
        self._orientation_angles = []

        faces = []

        if self.results.multi_face_landmarks:

            # Every faceLms corresponds to the landmarks of one face
            for faceLms in self.results.multi_face_landmarks:
                face_2d = []
                # 3D points are used internally to estimate head pose
                pose_2d = []
                pose_3d = []

                ih, iw, ic = img.shape

                for id, lm in enumerate(faceLms.landmark):

                    # Set aside some 2D - 3D correspondences of points that can
                    # be used to calculate the face orientation.
                    # Z-coordinate is scaled into real world coordinates
                    if pose_estimation and id in [33, 263, 1, 61, 291, 199]:
                        if id == 1:
                            x, y, z = (lm.x * iw), (lm.y * ih), lm.z * 3000

                        x, y, z = int(lm.x * iw), int(lm.y * ih), lm.z
                        pose_2d.append([x, y])
                        pose_3d.append([x, y, z])

                    # Landmark x,y,z is normalized
                    # Convert them back by remultiplying width/height
                    x, y = int(lm.x * iw), int(lm.y * ih)
                    face_2d.append([x, y])

                    if draw_points:
                        circle_radius = 2 if ih < 1000 else 6

                        if filtered:
                            if id in self.filtered_ids:
                                if draw_indices:
                                    cv2.putText(self._img, str(id), (x, y), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 0), 1)
                                else:
                                    cv2.circle(self._img, (x,y), circle_radius, (255,0,0), cv2.FILLED)
                        else:
                            if draw_indices:
                                cv2.putText(self._img, str(id), (x, y), cv2.FONT_HERSHEY_PLAIN, 0.7, (0, 255, 0), 1)
                            else:
                                cv2.circle(self._img, (x,y), circle_radius, (255,0,0), cv2.FILLED)
                

                if pose_estimation:
                    pose_2d = np.array(pose_2d, dtype=np.float64)
                    pose_3d = np.array(pose_3d, dtype=np.float64)

                    self._orientation_angles.append(self.pose_estimation(pose_2d, pose_3d))

                faces.append([face_2d[i] for i in self.filtered_ids] if filtered else face_2d)


                # Draw (filtered) landmarks
                if not draw_points:
                    self.mpDraw.draw_landmarks(self._img, faceLms, self.mpFaceMesh.FACEMESH_CONTOURS, self.drawSpec, self.drawSpec)

        return img, faces
    
    def pose_estimation(self, face2d, face3d, draw=True):
        """
        - face2d: 
        - face3d: 
        """

        ih, iw, _ = self._img.shape
        focal_length = 1 * iw
        cam_matrix = np.array([[focal_length, 0, ih / 2],
                               [0, focal_length, iw / 2],
                               [0, 0, 1]])


        nose2d = face2d[0]
        nose3d = face3d[0] 

        focal_length = 1 * iw

        cam_matrix = np.array([ [focal_length, 0, ih / 2],
                                [0, focal_length, iw / 2],
                                [0, 0, 1]])

        # The distortion parameters
        # Assuming no camer a distortion
        dist_matrix = np.zeros((4, 1), dtype=np.float64)

        # Solve PnP
        success, rot_vec, tvec = cv2.solvePnP(face3d, face2d, cam_matrix, dist_matrix)

        # Get rotational matrix
        rmat, jac = cv2.Rodrigues(rot_vec)

        # Get angles
        angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)

        # Get the y rotation degree
        x = angles[0] * 360
        y = angles[1] * 360
        z = angles[2] * 360

        if draw:
            # See where the user's head tilting
            if y < -10:
                text = "Looking Left"
            elif y > 10:
                text = "Looking Right"
            elif x < -10:
                text = "Looking Down"
            elif x > 10:
                text = "Looking Up"
            else:
                text = "Forward"

            p1 = (int(nose2d[0]), int(nose2d[1]))
            p2 = (int(nose2d[0] + y * 10) , int(nose2d[1] - x * 10))
            
            cv2.line(self._img, p1, p2, (255, 0, 0), 3)

            # Add the text on the image
            cv2.putText(self._img, text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 2)
            cv2.putText(self._img, "x: " + str(np.round(x,2)), (500, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            cv2.putText(self._img, "y: " + str(np.round(y,2)), (500, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            cv2.putText(self._img, "z: " + str(np.round(z,2)), (500, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        
        return [x, y, z]
    
    @property
    def orientation(self):
        return self._orientation_angles

# Sample usage
def main():
    use_video = True

    # Load the resource
    cap = cv2.VideoCapture(0) if use_video else cv2.imread('../../images/paralysis_test.jpg') 
    #cap = cv2.imread('/home/robbedec/repos/ugent/thesis-inwe/data/MEEI_Standard_Set/Flaccid/MildFlaccid/MildFlaccid1/MildFlaccid1_1.jpg')

    pTime = 0
    detector = MediapipeKPDetector(maxFaces=1)

    while True:
        if use_video:
            success, img = cap.read()
        else:
            img = cap

        # Call landmark generator
        img, faces = detector.findFaceMesh(img, draw_points=True, draw_indices=True, filtered=True, pose_estimation=True)

        # Calcultate FPS
        if use_video:
            cTime = time.time()
            fps = 1 / (cTime - pTime)
            pTime = cTime
            cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_PLAIN,
                        3, (0, 255, 0), 3)

        cv2.imshow("Image", img)

        if use_video:
            cv2.waitKey(1)
        else:
            cv2.waitKey(0)
            break

if __name__ == "__main__":
    main()